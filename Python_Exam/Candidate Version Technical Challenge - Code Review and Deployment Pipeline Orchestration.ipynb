{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0dad4",
   "metadata": {},
   "source": [
    "# Technical Challenge - Code Review and Deployment Pipeline Orchestration\n",
    "\n",
    "**Format:** Structured interview with whiteboarding/documentation  \n",
    "**Assessment Focus:** Problem decomposition, AI prompting strategy, system design\n",
    "\n",
    "**Please Fill in your Responses in the Response markdown boxes**\n",
    "\n",
    "---\n",
    "\n",
    "## Challenge Scenario\n",
    "\n",
    "You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:\n",
    "\n",
    "**Current Pain Points:**\n",
    "- Manual code reviews take 2-3 days per PR\n",
    "- Inconsistent review quality across teams\n",
    "- Deployment failures due to missed edge cases\n",
    "- Security vulnerabilities slip through reviews\n",
    "- No standardized deployment process across projects\n",
    "- Rollback decisions are manual and slow\n",
    "\n",
    "**Business Requirements:**\n",
    "- Reduce review time to <4 hours for standard PRs\n",
    "- Maintain or improve code quality\n",
    "- Catch 90%+ of security vulnerabilities before deployment\n",
    "- Standardize deployment across 50+ microservices\n",
    "- Enable automatic rollback based on metrics\n",
    "- Support multiple environments (dev, staging, prod)\n",
    "- Handle both new features and hotfixes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be761411",
   "metadata": {},
   "source": [
    "## Part A: Problem Decomposition (25 points)\n",
    "\n",
    "**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:\n",
    "- Clear input requirements\n",
    "- Specific output format\n",
    "- Success criteria\n",
    "- Failure handling strategy\n",
    "\n",
    "**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?\n",
    "\n",
    "**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a3c10",
   "metadata": {},
   "source": [
    "## Response Part A:\n",
    "\n",
    "\n",
    "### 1.1 Discrete Steps for AI/Automation\n",
    "- **Step 1: PR Submission Intake**  \n",
    "  - Input: Pull Request metadata, diff  \n",
    "  - Output: Record of PR in system, notification to reviewers or AI  \n",
    "  - Success: All PRs captured  \n",
    "  - Failure: Alert if PR not registered\n",
    "\n",
    "- **Step 2: Automated Code Analysis**  \n",
    "  - Input: Code diff  \n",
    "  - Output: Report with static analysis, lint, initial security checks  \n",
    "  - Success: Analyzes every file, flags issues  \n",
    "  - Failure: Fallback to manual notification\n",
    "\n",
    "- **Step 3: AI-Powered Code Review**  \n",
    "  - Input: Full code context, analysis output  \n",
    "  - Output: Structured review comments, risk/quality scores  \n",
    "  - Success: Submits review in <3 hours for standard PRs  \n",
    "  - Failure: Escalate to human review\n",
    "\n",
    "- **Step 4: Security and Regression Testing**  \n",
    "  - Input: Post-reviewed code  \n",
    "  - Output: Automated test results, code coverage, vulnerability scan  \n",
    "  - Success: ≥90% vulnerabilities caught  \n",
    "  - Failure: Block deployment, alert teams\n",
    "\n",
    "- **Step 5: Deployment Orchestration**  \n",
    "  - Input: Approved code, test results  \n",
    "  - Output: Deployed service, monitoring initialized  \n",
    "  - Success: Smooth, audited deployment  \n",
    "  - Failure: Auto-rollback, developer notification\n",
    "\n",
    "- **Step 6: Post-Deployment Monitoring**  \n",
    "  - Input: Deployment metrics  \n",
    "  - Output: Health checks, instant rollback if metrics degrade  \n",
    "  - Success: Rollout continues/stable  \n",
    "  - Failure: Rollback triggered automatically\n",
    "\n",
    "### 1.2 Parallel & Blocking Steps\n",
    "- Automated code analysis and initial security scan can run in parallel.  \n",
    "- AI review blocks deployment and must complete before proceeding.  \n",
    "- Deployment and monitoring are sequential, but monitoring can start alongside late-stage deployment.  \n",
    "- Key decisions: PR approval, security test outcome, live rollback.\n",
    "\n",
    "### 1.3 Key Handoffs/Data\n",
    "- PR metadata → automated analyzers  \n",
    "- Analyzer report → AI reviewer  \n",
    "- Review results + test artifacts → deployment system  \n",
    "- Deployment logs + metrics → monitoring/rollback manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38e9fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc377",
   "metadata": {},
   "source": [
    "## Part B: AI Prompting Strategy (30 points)\n",
    "\n",
    "**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:\n",
    "- System role/persona definition\n",
    "- Structured input format\n",
    "- Expected output format\n",
    "- Examples of good vs bad responses\n",
    "- Error handling instructions\n",
    "\n",
    "**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:\n",
    "- **Code that uses obscure libraries or frameworks**\n",
    "- **Security reviews for code**\n",
    "- **Performance analysis of database queries**\n",
    "- **Legacy code modifications**\n",
    "\n",
    "**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd049f5",
   "metadata": {},
   "source": [
    "## Response Part B:\n",
    "\n",
    "### 2.1 AI Prompts for Two Steps\n",
    "\n",
    "- **Step: Code Review**  \n",
    "  - Role: Senior Software Engineer mentor  \n",
    "  - Input: Code snippet, change context, previous review findings  \n",
    "  - Output: Structured review (positives, concerns, suggestions)  \n",
    "  - Example Good: “Functionality matches requirement, but edge conditions lack tests.”  \n",
    "  - Example Bad: “Code ok.”  \n",
    "  - Error: “Input incomplete”—flag for human.\n",
    "\n",
    "- **Step: Security Scan**  \n",
    "  - Role: Security Analyst  \n",
    "  - Input: Code diff, dependencies  \n",
    "  - Output: Table of vulnerabilities, threat severity, CVE links  \n",
    "  - Example Good: “SQL injection risk on line 40; recommend parameterized queries.”  \n",
    "  - Example Bad: “Looks safe.”  \n",
    "  - Error: “Unknown dependency”—require escalation.\n",
    "\n",
    "### 2.2 Handling Challenging Scenarios\n",
    "- Obscure Libraries: Lookup docs, fallback to ‘unknown,’ ask devs  \n",
    "- Security Reviews: Combine static scan with advisories, prompt for context if incomplete  \n",
    "- DB Query Performance: Request execution plan, sample data, metrics  \n",
    "- Legacy Modifications: Ask for code context and documentation, compare historic PRs\n",
    "\n",
    "### 2.3 Ensuring Prompt Effectiveness\n",
    "- Continuous monitoring of AI output quality vs. historic reviews  \n",
    "- A/B testing and developer feedback  \n",
    "- Calibration with sample error cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e98d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d353d",
   "metadata": {},
   "source": [
    "## Part C: System Architecture & Reusability (25 points)\n",
    "\n",
    "**Question 3.1:** How would you make this system reusable across different projects/teams? Consider:\n",
    "- Configuration management\n",
    "- Language/framework variations\n",
    "- Different deployment targets (cloud providers, on-prem)\n",
    "- Team-specific coding standards\n",
    "- Industry-specific compliance requirements\n",
    "\n",
    "**Question 3.2:** How would the system get better over time based on:\n",
    "- False positive/negative rates in reviews\n",
    "- Deployment success/failure patterns\n",
    "- Developer feedback\n",
    "- Production incident correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052f045",
   "metadata": {},
   "source": [
    "## Response Part C:\n",
    "\n",
    "### 3.1 Making the System Reusable\n",
    "- Modular config for targets: YAML/JSON for environment (cloud/on-prem)  \n",
    "- Adapter pattern for language and tool changes  \n",
    "- Plugin hooks for compliance (HIPAA, GDPR)  \n",
    "- Team-specific linter/ruleset loading  \n",
    "- Easy onboarding for new stacks\n",
    "\n",
    "### 3.2 System Improvement Over Time\n",
    "- Log false positives/negatives  \n",
    "- Trend analytics on deployment outcomes  \n",
    "- Feedback loops (surveys, review scoring)  \n",
    "- Retrain ML models on incident cause data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029f169",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d096eb",
   "metadata": {},
   "source": [
    "## Part D: Implementation Strategy (20 points)\n",
    "\n",
    "**Question 4.1:** Prioritize your implementation. What would you build first? Create a 6-month roadmap with:\n",
    "- MVP definition (what's the minimum viable system?)\n",
    "- Pilot program strategy\n",
    "- Rollout phases\n",
    "- Success metrics for each phase\n",
    "\n",
    "**Question 4.2:** Risk mitigation. What could go wrong and how would you handle:\n",
    "- AI making incorrect review decisions\n",
    "- System downtime during critical deployments\n",
    "- Integration failures with existing tools\n",
    "- Resistance from development teams\n",
    "- Compliance/audit requirements\n",
    "\n",
    "**Question 4.3:** Tool selection. What existing tools/platforms would you integrate with or build upon:\n",
    "- Code review platforms (GitHub, GitLab, Bitbucket)\n",
    "- CI/CD systems (Jenkins, GitHub Actions, GitLab CI)\n",
    "- Monitoring tools (Datadog, New Relic, Prometheus)\n",
    "- Security scanning tools (SonarQube, Snyk, Veracode)\n",
    "- Communication tools (Slack, Teams, Jira)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa9820",
   "metadata": {},
   "source": [
    "## Response Part D:\n",
    "\n",
    "### 4.1 Implementation Priorities & Roadmap\n",
    "- MVP: PR capture, static analysis, basic AI review, gated deployment for 1 service  \n",
    "- Pilot: Add security scanner, monitoring, onboard multiple teams  \n",
    "- Rollout: Expand to multiple microservices, integrate full auto-rollback  \n",
    "- Success Metrics: Review time <4 hrs, ≥90% security vulnerabilities caught, <2% deployment failure\n",
    "\n",
    "### 4.2 Risk Mitigation\n",
    "- Incorrect AI judgments: Human override, audits  \n",
    "- System downtime: Redundancy, manual fallback  \n",
    "- Integration failures: Staged rollout, test suite  \n",
    "- Developer resistance: Transparent communication, phased opt-in  \n",
    "- Compliance: Automated audit logs, configurable reports\n",
    "\n",
    "### 4.3 Tool Selection\n",
    "- Code review: GitHub, GitLab, Bitbucket  \n",
    "- CI/CD: Jenkins, GitHub Actions, GitLab CI  \n",
    "- Monitoring: Datadog, Prometheus, New Relic  \n",
    "- Security: SonarQube, Snyk, Veracode  \n",
    "- Communication: Slack, Jira, Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584added",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
